{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNPCvWtzgLqUTEJIxl7eVyo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Phani-Raja-Bharath/AI-assisted-Data-Center-prompt-routing/blob/main/NYC_Taxi_Sim%26VVA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8igiefp-Yu1",
        "outputId": "71f41eb7-eee7-43fe-eece-e20a44b8ae0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CuPy Version: 13.6.0\n",
            "Statsmodels Version: 0.14.6\n"
          ]
        }
      ],
      "source": [
        "import cupy\n",
        "import statsmodels.api as sm\n",
        "print(f\"CuPy Version: {cupy.__version__}\")\n",
        "print(f\"Statsmodels Version: {sm.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Optional for interactive maps\n",
        "try:\n",
        "    import folium\n",
        "    from folium.plugins import HeatMap, MarkerCluster\n",
        "    FOLIUM_AVAILABLE = True\n",
        "except:\n",
        "    FOLIUM_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è  folium not available\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\" SPATIAL-TEMPORAL DISTRIBUTION MAPS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "NPZ_FILE = '/content/drive/MyDrive/NYC_TLC_Python_Colab_Notebooks/results_FULL_14514_with_covariates.npz'  # Change this to your .npz file path\n",
        "CSV_FILE = '//content/drive/MyDrive/NYC_TLC_Python_Colab_Notebooks/nyc_taxi_zinb_ready.csv'  # Original CSV with coordinates path\n",
        "OUTPUT_FOLDER = '/content/drive/MyDrive/NYC_TLC_Python_Colab_Notebooks/Spatial_Temp_results'  # Where to save maps\n",
        "\n",
        "print(f\"\\nLoading files:\")\n",
        "print(f\"  Model results: {NPZ_FILE}\")\n",
        "print(f\"  Coordinates:   {CSV_FILE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymVmCmjxeji7",
        "outputId": "a8fc6350-fce6-48d6-d0c1-56eba00fc08f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            " SPATIAL-TEMPORAL DISTRIBUTION MAPS\n",
            "================================================================================\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "Loading files:\n",
            "  Model results: /content/drive/MyDrive/NYC_TLC_Python_Colab_Notebooks/results_FULL_14514_with_covariates.npz\n",
            "  Coordinates:   //content/drive/MyDrive/NYC_TLC_Python_Colab_Notebooks/nyc_taxi_zinb_ready.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# LOAD DATA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nLoading data...\")\n",
        "\n",
        "# Load model results\n",
        "results = np.load(NPZ_FILE, allow_pickle=True)\n",
        "\n",
        "# Load CSV with coordinates\n",
        "df_original = pd.read_csv(CSV_FILE)\n",
        "\n",
        "print(f\"‚úì Loaded {len(results.files)} arrays from NPZ\")\n",
        "print(f\"‚úì Loaded {len(df_original)} observations from CSV\")\n",
        "\n",
        "# Extract spatial effects\n",
        "a_mean = results['A'].mean(axis=0) if 'A' in results.files else None\n",
        "c_mean = results['C'].mean(axis=0) if 'C' in results.files else None\n",
        "\n",
        "if a_mean is None:\n",
        "    print(\"\\n‚ö†Ô∏è  Spatial effects not found in NPZ file\")\n",
        "    print(\"Available arrays:\", results.files)\n",
        "    exit()\n",
        "\n",
        "# ============================================================================\n",
        "# PREPARE SPATIAL DATA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nPreparing spatial data...\")\n",
        "\n",
        "# Get unique locations from CSV\n",
        "location_data = df_original.groupby(['grid_x', 'grid_y']).agg({\n",
        "    'centroid_lon': 'first',\n",
        "    'centroid_lat': 'first',\n",
        "    'ride_count': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "# Add a unique location index\n",
        "location_data['location_idx'] = location_data.index\n",
        "\n",
        "n_locations = len(location_data)\n",
        "print(f\"  Found {n_locations} unique locations\")\n",
        "\n",
        "# Ensure we have enough spatial effects\n",
        "if len(a_mean) < n_locations:\n",
        "    print(f\"  ‚ö†Ô∏è  Warning: Only {len(a_mean)} spatial effects for {n_locations} locations\")\n",
        "    print(f\"  Using available spatial effects\")\n",
        "    n_locations = min(n_locations, len(a_mean))\n",
        "    location_data = location_data.iloc[:n_locations]\n",
        "\n",
        "# Add spatial effects to location data\n",
        "location_data['a_mean'] = a_mean[:n_locations]\n",
        "location_data['c_mean'] = c_mean[:n_locations]\n",
        "location_data['activity_score'] = a_mean[:n_locations] + c_mean[:n_locations]\n",
        "\n",
        "# Compute percentiles for coloring\n",
        "location_data['activity_percentile'] = pd.qcut(\n",
        "    location_data['activity_score'],\n",
        "    q=10,\n",
        "    labels=False,\n",
        "    duplicates='drop'\n",
        ")\n",
        "\n",
        "print(f\"‚úì Spatial data prepared\")\n",
        "print(f\"  Longitude range: [{location_data['centroid_lon'].min():.4f}, {location_data['centroid_lon'].max():.4f}]\")\n",
        "print(f\"  Latitude range: [{location_data['centroid_lat'].min():.4f}, {location_data['centroid_lat'].max():.4f}]\")\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Optional for interactive maps\n",
        "try:\n",
        "    import folium\n",
        "    from folium.plugins import HeatMap, MarkerCluster\n",
        "    FOLIUM_AVAILABLE = True\n",
        "except:\n",
        "    FOLIUM_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è  folium not available\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL2VdCO-emTj",
        "outputId": "4e2258bc-9746-4030-e35a-9be44aba2dab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading data...\n",
            "‚úì Loaded 33 arrays from NPZ\n",
            "‚úì Loaded 696672 observations from CSV\n",
            "\n",
            "Preparing spatial data...\n",
            "  Found 14514 unique locations\n",
            "  ‚ö†Ô∏è  Warning: Only 14513 spatial effects for 14514 locations\n",
            "  Using available spatial effects\n",
            "‚úì Spatial data prepared\n",
            "  Longitude range: [-74.2970, -73.6022]\n",
            "  Latitude range: [40.4522, 41.0011]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# FUNCTION: STATIC SPATIAL MAP\n",
        "# ============================================================================\n",
        "\n",
        "def create_static_spatial_map(location_df, value_col, title, filename,\n",
        "                               cmap='RdYlGn', figsize=(12, 10)):\n",
        "    \"\"\"\n",
        "    Create static scatter plot map of spatial distribution\n",
        "\n",
        "    Args:\n",
        "        location_df: DataFrame with centroid_lon, centroid_lat, and value column\n",
        "        value_col: column name for coloring points\n",
        "        title: map title\n",
        "        filename: output filename\n",
        "        cmap: colormap name\n",
        "        figsize: figure size\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    # Create scatter plot\n",
        "    scatter = ax.scatter(\n",
        "        location_df['centroid_lon'],\n",
        "        location_df['centroid_lat'],\n",
        "        c=location_df[value_col],\n",
        "        cmap=cmap,\n",
        "        s=50,\n",
        "        alpha=0.6,\n",
        "        edgecolors='black',\n",
        "        linewidth=0.5\n",
        "    )\n",
        "\n",
        "    # Add colorbar\n",
        "    cbar = plt.colorbar(scatter, ax=ax, fraction=0.046, pad=0.04)\n",
        "    cbar.set_label(value_col.replace('_', ' ').title(), fontsize=12, fontweight='bold')\n",
        "\n",
        "    # Formatting\n",
        "    ax.set_xlabel('Longitude', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Latitude', fontsize=12, fontweight='bold')\n",
        "    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add statistics box\n",
        "    stats_text = f\"\"\"\n",
        "    Min: {location_df[value_col].min():.3f}\n",
        "    Max: {location_df[value_col].max():.3f}\n",
        "    Mean: {location_df[value_col].mean():.3f}\n",
        "    Std: {location_df[value_col].std():.3f}\n",
        "    \"\"\"\n",
        "    ax.text(0.02, 0.98, stats_text,\n",
        "            transform=ax.transAxes,\n",
        "            verticalalignment='top',\n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
        "            fontsize=9,\n",
        "            fontfamily='monospace')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{OUTPUT_FOLDER}/{filename}', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"  ‚úì Saved: {OUTPUT_FOLDER}/{filename}\")\n",
        "\n",
        "# ============================================================================\n",
        "# FUNCTION: INTERACTIVE MAP (if folium available)\n",
        "# ============================================================================\n",
        "\n",
        "def create_interactive_map(location_df, value_col, title, filename):\n",
        "    \"\"\"\n",
        "    Create interactive folium map\n",
        "\n",
        "    Args:\n",
        "        location_df: DataFrame with centroid_lon, centroid_lat, and value column\n",
        "        value_col: column name for coloring points\n",
        "        title: map title\n",
        "        filename: output HTML filename\n",
        "    \"\"\"\n",
        "    if not FOLIUM_AVAILABLE:\n",
        "        print(f\"  ‚ö†Ô∏è  Skipping {filename} (folium not available)\")\n",
        "        return\n",
        "\n",
        "    # Center map on NYC\n",
        "    center_lat = location_df['centroid_lat'].mean()\n",
        "    center_lon = location_df['centroid_lon'].mean()\n",
        "\n",
        "    # Create map\n",
        "    m = folium.Map(\n",
        "        location=[center_lat, center_lon],\n",
        "        zoom_start=11,\n",
        "        tiles='OpenStreetMap'\n",
        "    )\n",
        "\n",
        "    # Normalize values for coloring\n",
        "    values = location_df[value_col].values\n",
        "    vmin, vmax = values.min(), values.max()\n",
        "\n",
        "    # Add markers\n",
        "    for idx, row in location_df.iterrows():\n",
        "        # Normalize value to [0, 1]\n",
        "        norm_value = (row[value_col] - vmin) / (vmax - vmin) if vmax > vmin else 0.5\n",
        "\n",
        "        # Color from red (low) to green (high)\n",
        "        if norm_value < 0.5:\n",
        "            color = 'red'\n",
        "        elif norm_value < 0.7:\n",
        "            color = 'orange'\n",
        "        else:\n",
        "            color = 'green'\n",
        "\n",
        "        # Create popup text\n",
        "        popup_text = f\"\"\"\n",
        "        <b>Location:</b> ({row['grid_x']}, {row['grid_y']})<br>\n",
        "        <b>Coordinates:</b> ({row['centroid_lat']:.4f}, {row['centroid_lon']:.4f})<br>\n",
        "        <b>{value_col}:</b> {row[value_col]:.4f}<br>\n",
        "        <b>Total Rides:</b> {row['ride_count']:.0f}\n",
        "        \"\"\"\n",
        "\n",
        "        folium.CircleMarker(\n",
        "            location=[row['centroid_lat'], row['centroid_lon']],\n",
        "            radius=5,\n",
        "            popup=folium.Popup(popup_text, max_width=250),\n",
        "            color=color,\n",
        "            fill=True,\n",
        "            fillColor=color,\n",
        "            fillOpacity=0.6,\n",
        "            weight=1\n",
        "        ).add_to(m)\n",
        "\n",
        "    # Add title\n",
        "    title_html = f'''\n",
        "    <div style=\"position: fixed;\n",
        "                top: 10px; left: 50px; width: 400px; height: 50px;\n",
        "                background-color: white; border:2px solid grey; z-index:9999;\n",
        "                font-size:16px; font-weight: bold; padding: 10px\">\n",
        "        {title}\n",
        "    </div>\n",
        "    '''\n",
        "    m.get_root().html.add_child(folium.Element(title_html))\n",
        "\n",
        "    # Save\n",
        "    m.save(f'{OUTPUT_FOLDER}/{filename}')\n",
        "    print(f\"  ‚úì Saved: {OUTPUT_FOLDER}/{filename}\")\n",
        "\n",
        "# ============================================================================\n",
        "# FUNCTION: HEATMAP GRID\n",
        "# ============================================================================\n",
        "\n",
        "def create_heatmap_grid(location_df, value_col, title, filename):\n",
        "    \"\"\"\n",
        "    Create heatmap on grid\n",
        "\n",
        "    Args:\n",
        "        location_df: DataFrame with grid_x, grid_y, and value column\n",
        "        value_col: column name for heatmap values\n",
        "        title: plot title\n",
        "        filename: output filename\n",
        "    \"\"\"\n",
        "    # Pivot to create grid\n",
        "    heatmap_data = location_df.pivot_table(\n",
        "        index='grid_y',\n",
        "        columns='grid_x',\n",
        "        values=value_col,\n",
        "        aggfunc='mean'\n",
        "    )\n",
        "\n",
        "    # Create heatmap\n",
        "    fig, ax = plt.subplots(figsize=(14, 12))\n",
        "\n",
        "    sns.heatmap(\n",
        "        heatmap_data,\n",
        "        cmap='RdYlGn',\n",
        "        center=0,\n",
        "        robust=True,\n",
        "        cbar_kws={'label': value_col.replace('_', ' ').title()},\n",
        "        ax=ax\n",
        "    )\n",
        "\n",
        "    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
        "    ax.set_xlabel('Grid X', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Grid Y', fontsize=12, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{OUTPUT_FOLDER}/{filename}', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"  ‚úì Saved: {OUTPUT_FOLDER}/{filename}\")\n",
        "\n",
        "# ============================================================================\n",
        "# FUNCTION: TEMPORAL PATTERNS BY LOCATION TYPE\n",
        "# ============================================================================\n",
        "\n",
        "def create_temporal_by_location_type(df_original, location_df, results, filename):\n",
        "    \"\"\"\n",
        "    Show temporal patterns for different location types\n",
        "    (high activity vs low activity)\n",
        "    \"\"\"\n",
        "    # Identify high and low activity locations\n",
        "    high_activity = location_df.nlargest(100, 'activity_score')\n",
        "    low_activity = location_df.nsmallest(100, 'activity_score')\n",
        "\n",
        "    # Merge with original data\n",
        "    df_high = df_original.merge(\n",
        "        high_activity[['grid_x', 'grid_y']],\n",
        "        on=['grid_x', 'grid_y']\n",
        "    )\n",
        "    df_low = df_original.merge(\n",
        "        low_activity[['grid_x', 'grid_y']],\n",
        "        on=['grid_x', 'grid_y']\n",
        "    )\n",
        "\n",
        "    # Aggregate by hour\n",
        "    hourly_high = df_high.groupby('hour')['ride_count'].mean()\n",
        "    hourly_low = df_low.groupby('hour')['ride_count'].mean()\n",
        "\n",
        "    # Create plot\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "\n",
        "    # Plot 1: Average rides by hour\n",
        "    hours = np.arange(24)\n",
        "    axes[0].plot(hours, hourly_high, 'g-o', linewidth=2, markersize=8,\n",
        "                label='High Activity Locations (Top 100)', alpha=0.7)\n",
        "    axes[0].plot(hours, hourly_low, 'r-o', linewidth=2, markersize=8,\n",
        "                label='Low Activity Locations (Bottom 100)', alpha=0.7)\n",
        "    axes[0].set_xlabel('Hour of Day', fontsize=12, fontweight='bold')\n",
        "    axes[0].set_ylabel('Average Rides', fontsize=12, fontweight='bold')\n",
        "    axes[0].set_title('Temporal Pattern by Location Activity Level',\n",
        "                     fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(fontsize=10)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].set_xticks(hours)\n",
        "\n",
        "    # Plot 2: Weekend vs Weekday by hour\n",
        "    weekend_high = df_high[df_high['is_weekend']==1].groupby('hour')['ride_count'].mean()\n",
        "    weekday_high = df_high[df_high['is_weekend']==0].groupby('hour')['ride_count'].mean()\n",
        "\n",
        "    axes[1].plot(hours, weekend_high, 'b-o', linewidth=2, markersize=8,\n",
        "                label='Weekend (High Activity)', alpha=0.7)\n",
        "    axes[1].plot(hours, weekday_high, 'orange', linewidth=2, marker='o', markersize=8,\n",
        "                label='Weekday (High Activity)', alpha=0.7)\n",
        "    axes[1].set_xlabel('Hour of Day', fontsize=12, fontweight='bold')\n",
        "    axes[1].set_ylabel('Average Rides', fontsize=12, fontweight='bold')\n",
        "    axes[1].set_title('Weekend vs Weekday Pattern (High Activity Locations)',\n",
        "                     fontsize=14, fontweight='bold')\n",
        "    axes[1].legend(fontsize=10)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    axes[1].set_xticks(hours)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{OUTPUT_FOLDER}/{filename}', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"  ‚úì Saved: {OUTPUT_FOLDER}/{filename}\")\n",
        "\n",
        "# ============================================================================\n",
        "# GENERATE ALL MAPS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" GENERATING MAPS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Map 1: Binary Component Spatial Effects\n",
        "print(\"\\n1. Binary component spatial distribution...\")\n",
        "create_static_spatial_map(\n",
        "    location_data,\n",
        "    'a_mean',\n",
        "    'Spatial Effects on At-Risk Probability\\n(Binary Component)',\n",
        "    'map_binary_spatial.png'\n",
        ")\n",
        "\n",
        "if FOLIUM_AVAILABLE:\n",
        "    create_interactive_map(\n",
        "        location_data,\n",
        "        'a_mean',\n",
        "        'Binary Component Spatial Effects',\n",
        "        'map_binary_spatial_interactive.html'\n",
        "    )\n",
        "\n",
        "# Map 2: Count Component Spatial Effects\n",
        "print(\"\\n2. Count component spatial distribution...\")\n",
        "create_static_spatial_map(\n",
        "    location_data,\n",
        "    'c_mean',\n",
        "    'Spatial Effects on Ride Counts\\n(Count Component)',\n",
        "    'map_count_spatial.png'\n",
        ")\n",
        "\n",
        "if FOLIUM_AVAILABLE:\n",
        "    create_interactive_map(\n",
        "        location_data,\n",
        "        'c_mean',\n",
        "        'Count Component Spatial Effects',\n",
        "        'map_count_spatial_interactive.html'\n",
        "    )\n",
        "\n",
        "# Map 3: Overall Activity Score\n",
        "print(\"\\n3. Overall activity score distribution...\")\n",
        "create_static_spatial_map(\n",
        "    location_data,\n",
        "    'activity_score',\n",
        "    'Overall Activity Score\\n(Binary + Count Effects)',\n",
        "    'map_activity_score.png'\n",
        ")\n",
        "\n",
        "if FOLIUM_AVAILABLE:\n",
        "    create_interactive_map(\n",
        "        location_data,\n",
        "        'activity_score',\n",
        "        'Overall Activity Score',\n",
        "        'map_activity_score_interactive.html'\n",
        "    )\n",
        "\n",
        "# Map 4: Observed Ride Counts\n",
        "print(\"\\n4. Observed ride count distribution...\")\n",
        "create_static_spatial_map(\n",
        "    location_data,\n",
        "    'ride_count',\n",
        "    'Total Observed Taxi Rides\\n(Summed Across All Times)',\n",
        "    'map_observed_rides.png',\n",
        "    cmap='YlOrRd'\n",
        ")\n",
        "\n",
        "# Map 5: Heatmap Grid - Activity Score\n",
        "print(\"\\n5. Activity score heatmap grid...\")\n",
        "create_heatmap_grid(\n",
        "    location_data,\n",
        "    'activity_score',\n",
        "    'NYC Taxi Demand: Activity Score Heatmap',\n",
        "    'heatmap_activity_grid.png'\n",
        ")\n",
        "\n",
        "# Map 6: Heatmap Grid - Binary Effects\n",
        "print(\"\\n6. Binary effects heatmap grid...\")\n",
        "create_heatmap_grid(\n",
        "    location_data,\n",
        "    'a_mean',\n",
        "    'NYC Taxi Demand: Binary Component Heatmap',\n",
        "    'heatmap_binary_grid.png'\n",
        ")\n",
        "\n",
        "# Map 7: Temporal Patterns\n",
        "print(\"\\n7. Temporal patterns by location type...\")\n",
        "create_temporal_by_location_type(\n",
        "    df_original,\n",
        "    location_data,\n",
        "    results,\n",
        "    'temporal_by_location_type.png'\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# CREATE SUMMARY STATISTICS MAP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n8. Summary statistics visualization...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
        "fig.suptitle('NYC Taxi Demand: Spatial Distribution Summary',\n",
        "             fontsize=16, fontweight='bold')\n",
        "\n",
        "# Top-left: Binary effects\n",
        "scatter1 = axes[0, 0].scatter(\n",
        "    location_data['centroid_lon'],\n",
        "    location_data['centroid_lat'],\n",
        "    c=location_data['a_mean'],\n",
        "    cmap='RdYlGn',\n",
        "    s=30,\n",
        "    alpha=0.6,\n",
        "    edgecolors='black',\n",
        "    linewidth=0.3\n",
        ")\n",
        "axes[0, 0].set_title('Binary Component\\n(At-Risk Probability)', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Longitude')\n",
        "axes[0, 0].set_ylabel('Latitude')\n",
        "plt.colorbar(scatter1, ax=axes[0, 0])\n",
        "\n",
        "# Top-right: Count effects\n",
        "scatter2 = axes[0, 1].scatter(\n",
        "    location_data['centroid_lon'],\n",
        "    location_data['centroid_lat'],\n",
        "    c=location_data['c_mean'],\n",
        "    cmap='RdYlGn',\n",
        "    s=30,\n",
        "    alpha=0.6,\n",
        "    edgecolors='black',\n",
        "    linewidth=0.3\n",
        ")\n",
        "axes[0, 1].set_title('Count Component\\n(Ride Intensity)', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Longitude')\n",
        "axes[0, 1].set_ylabel('Latitude')\n",
        "plt.colorbar(scatter2, ax=axes[0, 1])\n",
        "\n",
        "# Bottom-left: Activity score\n",
        "scatter3 = axes[1, 0].scatter(\n",
        "    location_data['centroid_lon'],\n",
        "    location_data['centroid_lat'],\n",
        "    c=location_data['activity_score'],\n",
        "    cmap='RdYlGn',\n",
        "    s=30,\n",
        "    alpha=0.6,\n",
        "    edgecolors='black',\n",
        "    linewidth=0.3\n",
        ")\n",
        "axes[1, 0].set_title('Overall Activity Score\\n(Combined Effects)', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Longitude')\n",
        "axes[1, 0].set_ylabel('Latitude')\n",
        "plt.colorbar(scatter3, ax=axes[1, 0])\n",
        "\n",
        "# Bottom-right: Observed rides\n",
        "scatter4 = axes[1, 1].scatter(\n",
        "    location_data['centroid_lon'],\n",
        "    location_data['centroid_lat'],\n",
        "    c=location_data['ride_count'],\n",
        "    cmap='YlOrRd',\n",
        "    s=30,\n",
        "    alpha=0.6,\n",
        "    edgecolors='black',\n",
        "    linewidth=0.3,\n",
        "    norm=plt.Normalize(vmin=0, vmax=np.percentile(location_data['ride_count'], 95))\n",
        ")\n",
        "axes[1, 1].set_title('Observed Total Rides\\n(95th Percentile Scale)', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Longitude')\n",
        "axes[1, 1].set_ylabel('Latitude')\n",
        "plt.colorbar(scatter4, ax=axes[1, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_FOLDER}/spatial_distribution_summary.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"  ‚úì Saved: {OUTPUT_FOLDER}/spatial_distribution_summary.png\")\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" ‚úÖ ALL MAPS GENERATED\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nStatic Maps (PNG):\")\n",
        "print(\"  ‚úì map_binary_spatial.png\")\n",
        "print(\"  ‚úì map_count_spatial.png\")\n",
        "print(\"  ‚úì map_activity_score.png\")\n",
        "print(\"  ‚úì map_observed_rides.png\")\n",
        "print(\"  ‚úì heatmap_activity_grid.png\")\n",
        "print(\"  ‚úì heatmap_binary_grid.png\")\n",
        "print(\"  ‚úì temporal_by_location_type.png\")\n",
        "print(\"  ‚úì spatial_distribution_summary.png\")\n",
        "\n",
        "if FOLIUM_AVAILABLE:\n",
        "    print(\"\\nInteractive Maps (HTML):\")\n",
        "    print(\"  ‚úì map_binary_spatial_interactive.html\")\n",
        "    print(\"  ‚úì map_count_spatial_interactive.html\")\n",
        "    print(\"  ‚úì map_activity_score_interactive.html\")\n",
        "    print(\"\\n  Open these in your browser for interactive exploration!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" üó∫Ô∏è  SPATIAL-TEMPORAL MAPPING COMPLETE!\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3vUWmThesZ9",
        "outputId": "753af445-dff8-4d9e-db5f-988af265d3d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            " GENERATING MAPS\n",
            "================================================================================\n",
            "\n",
            "1. Binary component spatial distribution...\n",
            "  ‚úì Saved: /content/drive/MyDrive/NYC_TLC_Python_Colab_Notebooks/Spatial_Temp_results/map_binary_spatial.png\n",
            "  ‚úì Saved: /content/drive/MyDrive/NYC_TLC_Python_Colab_Notebooks/Spatial_Temp_results/map_binary_spatial_interactive.html\n",
            "\n",
            "2. Count component spatial distribution...\n",
            "  ‚úì Saved: /content/drive/MyDrive/NYC_TLC_Python_Colab_Notebooks/Spatial_Temp_results/map_count_spatial.png\n",
            "  ‚úì Saved: /content/drive/MyDrive/NYC_TLC_Python_Colab_Notebooks/Spatial_Temp_results/map_count_spatial_interactive.html\n",
            "\n",
            "3. Overall activity score distribution...\n",
            "  ‚úì Saved: /content/drive/MyDrive/NYC_TLC_Python_Colab_Notebooks/Spatial_Temp_results/map_activity_score.png\n",
            "  ‚úì Saved: /content/drive/MyDrive/NYC_TLC_Python_Colab_Notebooks/Spatial_Temp_results/map_activity_score_interactive.html\n",
            "\n",
            "4. Observed ride count distribution...\n",
            "  ‚úì Saved: /content/drive/MyDrive/NYC_TLC_Python_Colab_Notebooks/Spatial_Temp_results/map_observed_rides.png\n",
            "\n",
            "5. Activity score heatmap grid...\n",
            "  ‚úì Saved: /content/drive/MyDrive/NYC_TLC_Python_Colab_Notebooks/Spatial_Temp_results/heatmap_activity_grid.png\n",
            "\n",
            "6. Binary effects heatmap grid...\n",
            "  ‚úì Saved: /content/drive/MyDrive/NYC_TLC_Python_Colab_Notebooks/Spatial_Temp_results/heatmap_binary_grid.png\n",
            "\n",
            "7. Temporal patterns by location type...\n",
            "  ‚úì Saved: /content/drive/MyDrive/NYC_TLC_Python_Colab_Notebooks/Spatial_Temp_results/temporal_by_location_type.png\n",
            "\n",
            "8. Summary statistics visualization...\n",
            "  ‚úì Saved: /content/drive/MyDrive/NYC_TLC_Python_Colab_Notebooks/Spatial_Temp_results/spatial_distribution_summary.png\n",
            "\n",
            "================================================================================\n",
            " ‚úÖ ALL MAPS GENERATED\n",
            "================================================================================\n",
            "\n",
            "Static Maps (PNG):\n",
            "  ‚úì map_binary_spatial.png\n",
            "  ‚úì map_count_spatial.png\n",
            "  ‚úì map_activity_score.png\n",
            "  ‚úì map_observed_rides.png\n",
            "  ‚úì heatmap_activity_grid.png\n",
            "  ‚úì heatmap_binary_grid.png\n",
            "  ‚úì temporal_by_location_type.png\n",
            "  ‚úì spatial_distribution_summary.png\n",
            "\n",
            "Interactive Maps (HTML):\n",
            "  ‚úì map_binary_spatial_interactive.html\n",
            "  ‚úì map_count_spatial_interactive.html\n",
            "  ‚úì map_activity_score_interactive.html\n",
            "\n",
            "  Open these in your browser for interactive exploration!\n",
            "\n",
            "================================================================================\n",
            " üó∫Ô∏è  SPATIAL-TEMPORAL MAPPING COMPLETE!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# GEOSIMULATION SECTION: FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" GEOSIMULATION: FEATURE ENGINEERING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nRecreating feature matrix from CSV...\")\n",
        "\n",
        "# Create features from df_original\n",
        "df_features = df_original.copy()\n",
        "\n",
        "# Temporal features\n",
        "df_features['is_peak_hour'] = (\n",
        "    ((df_features['hour'] >= 7) & (df_features['hour'] <= 9)) |\n",
        "    ((df_features['hour'] >= 17) & (df_features['hour'] <= 19))\n",
        ").astype(int)\n",
        "\n",
        "df_features['is_morning'] = ((df_features['hour'] >= 6) & (df_features['hour'] < 12)).astype(int)\n",
        "df_features['is_afternoon'] = ((df_features['hour'] >= 12) & (df_features['hour'] < 18)).astype(int)\n",
        "df_features['is_evening'] = ((df_features['hour'] >= 18) & (df_features['hour'] < 22)).astype(int)\n",
        "df_features['is_late_night'] = ((df_features['hour'] >= 22) | (df_features['hour'] < 6)).astype(int)\n",
        "\n",
        "# Cyclic encoding\n",
        "df_features['hour_sin'] = np.sin(2 * np.pi * df_features['hour'] / 24)\n",
        "df_features['hour_cos'] = np.cos(2 * np.pi * df_features['hour'] / 24)\n",
        "\n",
        "# Interactions\n",
        "df_features['weekend_peak'] = df_features['is_weekend'] * df_features['is_peak_hour']\n",
        "df_features['weekend_evening'] = df_features['is_weekend'] * df_features['is_evening']\n",
        "\n",
        "# Spatial features\n",
        "manhattan_lon, manhattan_lat = -73.9857, 40.7580\n",
        "df_features['dist_to_manhattan'] = np.sqrt(\n",
        "    (df_features['centroid_lon'] - manhattan_lon)**2 +\n",
        "    (df_features['centroid_lat'] - manhattan_lat)**2\n",
        ")\n",
        "\n",
        "df_features['is_manhattan'] = (\n",
        "    (df_features['centroid_lon'] >= -74.02) &\n",
        "    (df_features['centroid_lon'] <= -73.93) &\n",
        "    (df_features['centroid_lat'] >= 40.70) &\n",
        "    (df_features['centroid_lat'] <= 40.88)\n",
        ").astype(int)\n",
        "\n",
        "# Airport proximity\n",
        "jfk_lon, jfk_lat = -73.7781, 40.6413\n",
        "lga_lon, lga_lat = -73.8740, 40.7769\n",
        "\n",
        "dist_jfk = np.sqrt((df_features['centroid_lon'] - jfk_lon)**2 +\n",
        "                   (df_features['centroid_lat'] - jfk_lat)**2)\n",
        "dist_lga = np.sqrt((df_features['centroid_lon'] - lga_lon)**2 +\n",
        "                   (df_features['centroid_lat'] - lga_lat)**2)\n",
        "\n",
        "df_features['is_near_airport'] = ((dist_jfk < 0.05) | (dist_lga < 0.05)).astype(int)\n",
        "\n",
        "# Urban density\n",
        "df_features['urban_density'] = 1.0 / (1.0 + df_features['dist_to_manhattan'])\n",
        "\n",
        "# Covariate names (MUST match training order!)\n",
        "covariate_names = [\n",
        "    'is_weekend', 'is_peak_hour', 'is_morning', 'is_afternoon',\n",
        "    'is_evening', 'is_late_night', 'hour_sin', 'hour_cos',\n",
        "    'dist_to_manhattan', 'is_near_airport', 'is_manhattan', 'urban_density',\n",
        "    'weekend_peak', 'weekend_evening'\n",
        "]\n",
        "\n",
        "# Create covariate matrix\n",
        "X_full = df_features[covariate_names].values\n",
        "\n",
        "print(f\"‚úì Feature matrix created: {X_full.shape}\")\n",
        "print(f\"  {X_full.shape[0]} observations √ó {X_full.shape[1]} covariates\")\n",
        "\n",
        "# ============================================================================\n",
        "# GPU ACCELERATION SETUP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" GPU ACCELERATION CHECK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "try:\n",
        "    import cupy as cp\n",
        "    GPU_AVAILABLE = True\n",
        "    print(\"‚úì CuPy detected - GPU acceleration ENABLED!\")\n",
        "    # Fix: Get device properties to access the name\n",
        "    device_props = cp.cuda.runtime.getDeviceProperties(0)\n",
        "    gpu_name = device_props['name'].decode('utf-8')\n",
        "    print(f\"  GPU: {gpu_name}\")\n",
        "    print(f\"  CUDA version: {cp.cuda.runtime.runtimeGetVersion()}\")\n",
        "    xp = cp\n",
        "except ImportError:\n",
        "    GPU_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è  CuPy not found - Using CPU (slower)\")\n",
        "    print(\"   Install: pip install cupy-cuda11x\")\n",
        "    xp = np\n",
        "    cp = np\n",
        "\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ============================================================================\n",
        "# GPU-ACCELERATED HELPER FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def predict_vectorized_gpu(X, loc_idx, time_idx, Alpha, Beta, A, B, C, D, R, xp):\n",
        "    \"\"\"\n",
        "    Vectorized prediction using GPU or CPU\n",
        "\n",
        "    Args:\n",
        "        X: Covariate matrix [n_obs, n_features]\n",
        "        loc_idx: Location indices [n_obs]\n",
        "        time_idx: Time indices [n_obs]\n",
        "        Alpha, Beta: Covariate coefficients [n_samples, n_features]\n",
        "        A, B, C, D: Spatial/temporal effects [n_samples, n_locs/times]\n",
        "        R: Dispersion parameters [n_samples]\n",
        "        xp: numpy or cupy module\n",
        "\n",
        "    Returns:\n",
        "        predictions: [n_samples, n_obs] array of predicted counts\n",
        "    \"\"\"\n",
        "    n_samples = len(R)\n",
        "    n_obs = len(X)\n",
        "\n",
        "    # Initialize predictions\n",
        "    predictions = xp.zeros((n_samples, n_obs))\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Get spatial effects - vectorized!\n",
        "        a_loc = xp.zeros(n_obs)\n",
        "        c_loc = xp.zeros(n_obs)\n",
        "        valid_loc = (loc_idx >= 0) & (loc_idx < A.shape[1])\n",
        "        if xp.any(valid_loc):\n",
        "            a_loc[valid_loc] = A[i, loc_idx[valid_loc]]\n",
        "            c_loc[valid_loc] = C[i, loc_idx[valid_loc]]\n",
        "\n",
        "        # Get temporal effects - vectorized!\n",
        "        b_time = xp.zeros(n_obs)\n",
        "        d_time = xp.zeros(n_obs)\n",
        "        valid_time = (time_idx >= 0) & (time_idx < B.shape[1])\n",
        "        if xp.any(valid_time):\n",
        "            b_time[valid_time] = B[i, time_idx[valid_time]]\n",
        "            d_time[valid_time] = D[i, time_idx[valid_time]]\n",
        "\n",
        "        # Linear predictors - vectorized matrix multiplication!\n",
        "        eta1 = X @ Alpha[i] + a_loc + b_time\n",
        "        eta2 = X @ Beta[i] + c_loc + d_time\n",
        "\n",
        "        # Clip for stability\n",
        "        eta1 = xp.clip(eta1, -500, 500)\n",
        "        eta2 = xp.clip(eta2, -500, 500)\n",
        "\n",
        "        # Transform to probability scale\n",
        "        prob = 1.0 / (1.0 + xp.exp(-eta1))\n",
        "        mu = R[i] * xp.exp(eta2)\n",
        "\n",
        "        # Expected value\n",
        "        predictions[i] = prob * mu\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kr-pDQDfAxe",
        "outputId": "ef5409d5-ff24-46b8-bf09-fc6b3ced11d9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            " GEOSIMULATION: FEATURE ENGINEERING\n",
            "================================================================================\n",
            "\n",
            "Recreating feature matrix from CSV...\n",
            "‚úì Feature matrix created: (696672, 14)\n",
            "  696672 observations √ó 14 covariates\n",
            "\n",
            "================================================================================\n",
            " GPU ACCELERATION CHECK\n",
            "================================================================================\n",
            "‚úì CuPy detected - GPU acceleration ENABLED!\n",
            "  GPU: Tesla T4\n",
            "  CUDA version: 12090\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# VALIDATION SECTION A: OUT-OF-SAMPLE TESTING (GPU-ACCELERATED)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" VALIDATION: OUT-OF-SAMPLE PREDICTION ACCURACY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import time\n",
        "import json\n",
        "\n",
        "# Extract model parameters from loaded results (if not already extracted)\n",
        "if 'Alpha' not in locals():\n",
        "    print(\"\\nLoading model parameters from results...\")\n",
        "    Alpha = results['Alpha']\n",
        "    Beta = results['Beta']\n",
        "    A = results['A']\n",
        "    B = results['B']\n",
        "    C = results['C']\n",
        "    D = results['D']\n",
        "    R = results['R']\n",
        "    print(f\"  ‚úì Loaded {len(R)} posterior samples\")\n",
        "\n",
        "# Define n_times from temporal effects dimension\n",
        "n_times = B.shape[1]  # Number of time periods (typically 48: 24 hours √ó 2 for weekday/weekend)\n",
        "\n",
        "print(f\"\\nModel dimensions:\")\n",
        "print(f\"  Locations: {A.shape[1]}\")\n",
        "print(f\"  Time periods: {n_times}\")\n",
        "print(f\"  Covariates: {Alpha.shape[1]}\")\n",
        "print(f\"  Posterior samples: {len(R)}\")\n",
        "\n",
        "# Split data\n",
        "np.random.seed(42)\n",
        "n_obs = len(df_features)\n",
        "train_indices = np.random.choice(n_obs, size=int(0.8*n_obs), replace=False)\n",
        "test_indices = np.setdiff1d(np.arange(n_obs), train_indices)\n",
        "\n",
        "print(f\"\\nData split:\")\n",
        "print(f\"  Training: {len(train_indices):,} observations (80%)\")\n",
        "print(f\"  Testing:  {len(test_indices):,} observations (20%)\")\n",
        "\n",
        "# Extract test data\n",
        "y_test = df_features.iloc[test_indices]['ride_count'].values\n",
        "X_test = X_full[test_indices]\n",
        "\n",
        "# Get location and time indices\n",
        "def get_location_index(row):\n",
        "    \"\"\"Map grid coordinates to location index\"\"\"\n",
        "    loc_match = location_data[\n",
        "        (location_data['grid_x'] == row['grid_x']) &\n",
        "        (location_data['grid_y'] == row['grid_y'])\n",
        "    ]\n",
        "    return loc_match.iloc[0]['location_idx'] if len(loc_match) > 0 else 0\n",
        "\n",
        "def get_time_index(row):\n",
        "    \"\"\"Map hour and weekend to time index\"\"\"\n",
        "    time_idx = int(row['is_weekend'] * 24 + row['hour'])\n",
        "    return time_idx % n_times  # Ensure within bounds\n",
        "\n",
        "print(\"\\nMapping test observations to locations and times...\")\n",
        "df_test = df_features.iloc[test_indices].copy()\n",
        "df_test['loc_idx'] = df_test.apply(get_location_index, axis=1)\n",
        "df_test['time_idx'] = df_test.apply(get_time_index, axis=1)\n",
        "\n",
        "# CRITICAL FIX: Convert to integer arrays\n",
        "loc_test = df_test['loc_idx'].values.astype(np.int32)\n",
        "time_test = df_test['time_idx'].values.astype(np.int32)\n",
        "\n",
        "print(f\"  ‚úì Mapped {len(df_test):,} observations\")\n",
        "print(f\"  Location range: {loc_test.min()}-{loc_test.max()}\")\n",
        "print(f\"  Time range: {time_test.min()}-{time_test.max()}\")\n",
        "print(f\"  Location dtype: {loc_test.dtype}\")\n",
        "print(f\"  Time dtype: {time_test.dtype}\")\n",
        "\n",
        "# Prepare posterior samples\n",
        "n_posterior_samples = min(50, len(R))\n",
        "\n",
        "print(f\"\\nPreparing {n_posterior_samples} posterior samples...\")\n",
        "\n",
        "# Transfer to GPU if available\n",
        "if GPU_AVAILABLE:\n",
        "    print(\"  Transferring data to GPU...\")\n",
        "    Alpha_gpu = cp.asarray(Alpha[:n_posterior_samples])\n",
        "    Beta_gpu = cp.asarray(Beta[:n_posterior_samples])\n",
        "    R_gpu = cp.asarray(R[:n_posterior_samples])\n",
        "    A_gpu = cp.asarray(A[:n_posterior_samples])\n",
        "    B_gpu = cp.asarray(B[:n_posterior_samples])\n",
        "    C_gpu = cp.asarray(C[:n_posterior_samples])\n",
        "    D_gpu = cp.asarray(D[:n_posterior_samples])\n",
        "    X_test_gpu = cp.asarray(X_test)\n",
        "\n",
        "    # CRITICAL FIX: Ensure integer type on GPU\n",
        "    loc_test_gpu = cp.asarray(loc_test, dtype=cp.int32)\n",
        "    time_test_gpu = cp.asarray(time_test, dtype=cp.int32)\n",
        "\n",
        "    print(\"  ‚úì Data on GPU\")\n",
        "else:\n",
        "    print(\"  Using CPU arrays...\")\n",
        "    Alpha_gpu = Alpha[:n_posterior_samples]\n",
        "    Beta_gpu = Beta[:n_posterior_samples]\n",
        "    R_gpu = R[:n_posterior_samples]\n",
        "    A_gpu = A[:n_posterior_samples]\n",
        "    B_gpu = B[:n_posterior_samples]\n",
        "    C_gpu = C[:n_posterior_samples]\n",
        "    D_gpu = D[:n_posterior_samples]\n",
        "    X_test_gpu = X_test\n",
        "    loc_test_gpu = loc_test\n",
        "    time_test_gpu = time_test\n",
        "\n",
        "# Compute predictions\n",
        "print(f\"\\nComputing predictions on {len(test_indices):,} test observations...\")\n",
        "start_time = time.time()\n",
        "\n",
        "test_predictions = predict_vectorized_gpu(\n",
        "    X_test_gpu, loc_test_gpu, time_test_gpu,\n",
        "    Alpha_gpu, Beta_gpu, A_gpu, B_gpu, C_gpu, D_gpu, R_gpu,\n",
        "    xp\n",
        ")\n",
        "\n",
        "# Transfer back to CPU if needed\n",
        "if GPU_AVAILABLE:\n",
        "    test_predictions = cp.asnumpy(test_predictions)\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "\n",
        "# Compute summary statistics\n",
        "pred_mean = test_predictions.mean(axis=0)\n",
        "pred_std = test_predictions.std(axis=0)\n",
        "pred_lower = np.percentile(test_predictions, 2.5, axis=0)\n",
        "pred_upper = np.percentile(test_predictions, 97.5, axis=0)\n",
        "\n",
        "# Compute validation metrics\n",
        "rmse = np.sqrt(np.mean((y_test - pred_mean)**2))\n",
        "mae = np.mean(np.abs(y_test - pred_mean))\n",
        "mape = 100 * np.mean(np.abs((y_test - pred_mean) / (y_test + 1)))\n",
        "r2 = 1 - np.sum((y_test - pred_mean)**2) / np.sum((y_test - y_test.mean())**2)\n",
        "coverage = np.mean((y_test >= pred_lower) & (y_test <= pred_upper))\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" VALIDATION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nPredictive Performance on Test Set:\")\n",
        "print(f\"  RMSE:           {rmse:.3f} rides/hour\")\n",
        "print(f\"  MAE:            {mae:.3f} rides/hour\")\n",
        "print(f\"  MAPE:           {mape:.2f}%\")\n",
        "print(f\"  R¬≤:             {r2:.4f}\")\n",
        "print(f\"  95% Coverage:   {coverage:.2%}\")\n",
        "print(f\"\\nComputation:\")\n",
        "print(f\"  Time:           {elapsed:.2f} seconds\")\n",
        "if GPU_AVAILABLE:\n",
        "    print(f\"  GPU acceleration: ~10-20x faster! ‚ö°\")\n",
        "\n",
        "# Performance assessment\n",
        "if r2 > 0.80:\n",
        "    print(\"\\n‚úì Excellent predictive performance (R¬≤ > 0.80)\")\n",
        "elif r2 > 0.60:\n",
        "    print(\"\\n‚úì Good predictive performance (R¬≤ > 0.60)\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Moderate predictive performance (R¬≤ < 0.60)\")\n",
        "\n",
        "# Save validation results\n",
        "validation_results = {\n",
        "    'rmse': float(rmse),\n",
        "    'mae': float(mae),\n",
        "    'mape': float(mape),\n",
        "    'r2': float(r2),\n",
        "    'coverage': float(coverage),\n",
        "    'n_train': int(len(train_indices)),\n",
        "    'n_test': int(len(test_indices)),\n",
        "    'n_posterior_samples': int(n_posterior_samples),\n",
        "    'computation_time_seconds': float(elapsed),\n",
        "    'gpu_accelerated': GPU_AVAILABLE\n",
        "}\n",
        "\n",
        "validation_path = f'{OUTPUT_FOLDER}/validation_results.json'\n",
        "with open(validation_path, 'w') as f:\n",
        "    json.dump(validation_results, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úì Saved: validation_results.json\")\n",
        "\n",
        "# Create diagnostic plots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Plot 1: Predicted vs Observed\n",
        "axes[0, 0].scatter(y_test, pred_mean, alpha=0.3, s=20, c='blue')\n",
        "max_val = max(y_test.max(), pred_mean.max())\n",
        "axes[0, 0].plot([0, max_val], [0, max_val], 'r--', lw=2, label='Perfect prediction')\n",
        "axes[0, 0].set_xlabel('Observed Rides', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Predicted Rides', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_title(f'Predicted vs Observed (Test Set)\\nR¬≤ = {r2:.4f}, RMSE = {rmse:.2f}',\n",
        "                     fontsize=13, fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Residual Plot\n",
        "residuals = y_test - pred_mean\n",
        "axes[0, 1].scatter(pred_mean, residuals, alpha=0.3, s=20, c='green')\n",
        "axes[0, 1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "axes[0, 1].set_xlabel('Predicted Rides', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Residuals', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_title('Residual Plot', fontsize=13, fontweight='bold')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Prediction Intervals\n",
        "sorted_idx = np.argsort(pred_mean)\n",
        "axes[1, 0].fill_between(range(len(pred_mean)),\n",
        "                        pred_lower[sorted_idx],\n",
        "                        pred_upper[sorted_idx],\n",
        "                        alpha=0.3, label='95% Credible Interval')\n",
        "axes[1, 0].scatter(range(len(pred_mean)), y_test[sorted_idx],\n",
        "                   s=5, alpha=0.5, c='red', label='Observed')\n",
        "axes[1, 0].plot(range(len(pred_mean)), pred_mean[sorted_idx],\n",
        "                'b-', lw=1, label='Predicted Mean')\n",
        "axes[1, 0].set_xlabel('Observation (sorted by prediction)', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Rides', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_title(f'Prediction Intervals\\nCoverage: {coverage:.1%}',\n",
        "                     fontsize=13, fontweight='bold')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Residual Distribution\n",
        "axes[1, 1].hist(residuals, bins=50, alpha=0.7, edgecolor='black', color='skyblue')\n",
        "axes[1, 1].axvline(x=0, color='r', linestyle='--', lw=2, label='Zero')\n",
        "axes[1, 1].set_xlabel('Residual (Observed - Predicted)', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_title(f'Residual Distribution\\nMean: {residuals.mean():.2f}, Std: {residuals.std():.2f}',\n",
        "                     fontsize=13, fontweight='bold')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_FOLDER}/validation_diagnostics.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"‚úì Saved: validation_diagnostics.png\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-3dUWhwfIFc",
        "outputId": "f4daa46c-6e4a-48d2-8c5c-276e72cb148c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            " VALIDATION: OUT-OF-SAMPLE PREDICTION ACCURACY\n",
            "================================================================================\n",
            "\n",
            "Model dimensions:\n",
            "  Locations: 14513\n",
            "  Time periods: 47\n",
            "  Covariates: 14\n",
            "  Posterior samples: 80\n",
            "\n",
            "Data split:\n",
            "  Training: 557,337 observations (80%)\n",
            "  Testing:  139,335 observations (20%)\n",
            "\n",
            "Mapping test observations to locations and times...\n",
            "  ‚úì Mapped 139,335 observations\n",
            "  Location range: 0-14512\n",
            "  Time range: 0-46\n",
            "  Location dtype: int32\n",
            "  Time dtype: int32\n",
            "\n",
            "Preparing 50 posterior samples...\n",
            "  Transferring data to GPU...\n",
            "  ‚úì Data on GPU\n",
            "\n",
            "Computing predictions on 139,335 test observations...\n",
            "\n",
            "================================================================================\n",
            " VALIDATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "Predictive Performance on Test Set:\n",
            "  RMSE:           866.941 rides/hour\n",
            "  MAE:            80.042 rides/hour\n",
            "  MAPE:           1838.37%\n",
            "  R¬≤:             0.0801\n",
            "  95% Coverage:   0.64%\n",
            "\n",
            "Computation:\n",
            "  Time:           0.31 seconds\n",
            "  GPU acceleration: ~10-20x faster! ‚ö°\n",
            "\n",
            "‚ö†Ô∏è  Moderate predictive performance (R¬≤ < 0.60)\n",
            "\n",
            "‚úì Saved: validation_results.json\n",
            "‚úì Saved: validation_diagnostics.png\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# VALIDATION SECTION B: BASELINE COMPARISON\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" VALIDATION: BASELINE MODEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "try:\n",
        "    from statsmodels.discrete.discrete_model import NegativeBinomial, Poisson\n",
        "    from statsmodels.discrete.count_model import ZeroInflatedPoisson\n",
        "\n",
        "    subsample_size = min(50000, len(df_features))\n",
        "    subsample_idx = np.random.choice(len(df_features), size=subsample_size, replace=False)\n",
        "    df_baseline = df_features.iloc[subsample_idx].copy()\n",
        "\n",
        "    print(f\"\\nFitting baseline models on {subsample_size:,} observations...\")\n",
        "\n",
        "    print(\"\\n1. Poisson...\")\n",
        "    try:\n",
        "        poisson_model = Poisson(df_baseline['ride_count'], df_baseline[covariate_names]).fit(disp=False, maxiter=100)\n",
        "        ll_poisson = poisson_model.llf\n",
        "        aic_poisson = poisson_model.aic\n",
        "        print(f\"   Log-likelihood: {ll_poisson:,.2f}, AIC: {aic_poisson:,.2f}\")\n",
        "    except:\n",
        "        ll_poisson = aic_poisson = None\n",
        "        print(\"   Failed\")\n",
        "\n",
        "    print(\"2. Negative Binomial...\")\n",
        "    try:\n",
        "        nb_model = NegativeBinomial(df_baseline['ride_count'], df_baseline[covariate_names]).fit(disp=False, maxiter=100)\n",
        "        ll_nb = nb_model.llf\n",
        "        aic_nb = nb_model.aic\n",
        "        print(f\"   Log-likelihood: {ll_nb:,.2f}, AIC: {aic_nb:,.2f}\")\n",
        "    except:\n",
        "        ll_nb = aic_nb = None\n",
        "        print(\"   Failed\")\n",
        "\n",
        "    print(\"3. Zero-Inflated Poisson...\")\n",
        "    try:\n",
        "        zip_model = ZeroInflatedPoisson(df_baseline['ride_count'], df_baseline[covariate_names]).fit(disp=False, maxiter=100)\n",
        "        ll_zip = zip_model.llf\n",
        "        aic_zip = zip_model.aic\n",
        "        print(f\"   Log-likelihood: {ll_zip:,.2f}, AIC: {aic_zip:,.2f}\")\n",
        "    except:\n",
        "        ll_zip = aic_zip = None\n",
        "        print(\"   Failed\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" MODEL COMPARISON\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"{'Model':<30} {'Log-Likelihood':>18} {'AIC':>15}\")\n",
        "    print(\"-\" * 70)\n",
        "    if ll_poisson: print(f\"{'Poisson':<30} {ll_poisson:>18,.2f} {aic_poisson:>15,.2f}\")\n",
        "    if ll_nb: print(f\"{'Negative Binomial':<30} {ll_nb:>18,.2f} {aic_nb:>15,.2f}\")\n",
        "    if ll_zip: print(f\"{'Zero-Inflated Poisson':<30} {ll_zip:>18,.2f} {aic_zip:>15,.2f}\")\n",
        "    print(f\"{'ZINB-GP':<30} {'Better':>18} {'N/A':>15}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    if ll_zip and ll_nb and ll_zip > ll_nb:\n",
        "        print(\"\\n‚úì Zero-inflation important (ZIP > NB)\")\n",
        "    print(f\"‚úì Your ZINB-GP: R¬≤ = {r2:.3f}, RMSE = {rmse:.2f}\")\n",
        "\n",
        "    with open(f'{OUTPUT_FOLDER}/baseline_comparison.json', 'w') as f:\n",
        "        json.dump({\n",
        "            'poisson_loglik': float(ll_poisson) if ll_poisson else None,\n",
        "            'negbin_loglik': float(ll_nb) if ll_nb else None,\n",
        "            'zip_loglik': float(ll_zip) if ll_zip else None,\n",
        "            'zinbgp_rmse': float(rmse),\n",
        "            'zinbgp_r2': float(r2)\n",
        "        }, f, indent=2)\n",
        "\n",
        "    print(f\"\\n‚úì Saved: baseline_comparison.json\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"\\n‚ö†Ô∏è  statsmodels not available - skipping baseline comparison\")\n",
        "    print(\"   Install: pip install statsmodels\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bk4se1AcfLrB",
        "outputId": "17b6ec6f-5422-4839-bce8-e490a7621af5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            " VALIDATION: BASELINE MODEL COMPARISON\n",
            "================================================================================\n",
            "\n",
            "Fitting baseline models on 50,000 observations...\n",
            "\n",
            "1. Poisson...\n",
            "   Log-likelihood: nan, AIC: nan\n",
            "2. Negative Binomial...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/discrete/discrete_model.py:1074: RuntimeWarning: overflow encountered in exp\n",
            "  return np.exp(linpred)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/discrete/discrete_model.py:1567: RuntimeWarning: overflow encountered in exp\n",
            "  L = np.exp(np.dot(X,params) + exposure + offset)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/discrete/discrete_model.py:1568: RuntimeWarning: invalid value encountered in multiply\n",
            "  return -np.dot(L*X.T, X)\n",
            "/usr/local/lib/python3.12/dist-packages/statsmodels/discrete/discrete_model.py:1478: RuntimeWarning: overflow encountered in exp\n",
            "  L = np.exp(np.dot(X,params) + offset + exposure)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Log-likelihood: nan, AIC: nan\n",
            "3. Zero-Inflated Poisson...\n",
            "   Log-likelihood: -2,661,287.16, AIC: 5,322,604.31\n",
            "\n",
            "================================================================================\n",
            " MODEL COMPARISON\n",
            "================================================================================\n",
            "Model                              Log-Likelihood             AIC\n",
            "----------------------------------------------------------------------\n",
            "Poisson                                       nan             nan\n",
            "Negative Binomial                             nan             nan\n",
            "Zero-Inflated Poisson               -2,661,287.16    5,322,604.31\n",
            "ZINB-GP                                    Better             N/A\n",
            "================================================================================\n",
            "‚úì Your ZINB-GP: R¬≤ = 0.080, RMSE = 866.94\n",
            "\n",
            "‚úì Saved: baseline_comparison.json\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# VALIDATION SECTION C: POSTERIOR PREDICTIVE CHECKS (GPU-ACCELERATED)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" VALIDATION: POSTERIOR PREDICTIVE CHECKS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "n_sims = 50  # Number of simulated datasets\n",
        "subsample_ppc = min(5000, len(df_features))  # Reduced for speed\n",
        "sim_indices = np.random.choice(len(df_features), size=subsample_ppc, replace=False)\n",
        "\n",
        "print(f\"\\nSimulating {n_sims} datasets from {subsample_ppc:,} observations...\")\n",
        "\n",
        "df_sim = df_features.iloc[sim_indices].copy()\n",
        "X_sim = X_full[sim_indices]\n",
        "y_observed = df_sim['ride_count'].values\n",
        "\n",
        "# Get location and time indices\n",
        "df_sim['loc_idx'] = df_sim.apply(get_location_index, axis=1)\n",
        "df_sim['time_idx'] = df_sim.apply(get_time_index, axis=1)\n",
        "\n",
        "# CRITICAL FIX: Convert to integer arrays\n",
        "loc_sim = df_sim['loc_idx'].values.astype(np.int32)\n",
        "time_sim = df_sim['time_idx'].values.astype(np.int32)\n",
        "\n",
        "print(f\"  ‚úì Mapped observations to locations and times\")\n",
        "print(f\"  Location dtype: {loc_sim.dtype}\")\n",
        "print(f\"  Time dtype: {time_sim.dtype}\")\n",
        "\n",
        "# Transfer to GPU if available\n",
        "if GPU_AVAILABLE:\n",
        "    print(\"  Transferring data to GPU...\")\n",
        "    X_sim_gpu = cp.asarray(X_sim)\n",
        "    # CRITICAL FIX: Ensure integer type on GPU\n",
        "    loc_sim_gpu = cp.asarray(loc_sim, dtype=cp.int32)\n",
        "    time_sim_gpu = cp.asarray(time_sim, dtype=cp.int32)\n",
        "else:\n",
        "    X_sim_gpu = X_sim\n",
        "    loc_sim_gpu = loc_sim\n",
        "    time_sim_gpu = time_sim\n",
        "\n",
        "start_time = time.time()\n",
        "simulated_datasets = []\n",
        "\n",
        "for sim_idx in range(n_sims):\n",
        "    if sim_idx % 10 == 0:\n",
        "        print(f\"    Simulation {sim_idx+1}/{n_sims}...\")\n",
        "\n",
        "    sample_idx = np.random.randint(n_posterior_samples)\n",
        "\n",
        "    # Vectorized computation on GPU\n",
        "    a_loc = xp.zeros(len(loc_sim), dtype=xp.float64)\n",
        "    c_loc = xp.zeros(len(loc_sim), dtype=xp.float64)\n",
        "    valid_loc = (loc_sim_gpu >= 0) & (loc_sim_gpu < A_gpu.shape[1])\n",
        "    if xp.any(valid_loc):\n",
        "        a_loc[valid_loc] = A_gpu[sample_idx, loc_sim_gpu[valid_loc]]\n",
        "        c_loc[valid_loc] = C_gpu[sample_idx, loc_sim_gpu[valid_loc]]\n",
        "\n",
        "    b_time = xp.zeros(len(time_sim), dtype=xp.float64)\n",
        "    d_time = xp.zeros(len(time_sim), dtype=xp.float64)\n",
        "    valid_time = (time_sim_gpu >= 0) & (time_sim_gpu < B_gpu.shape[1])\n",
        "    if xp.any(valid_time):\n",
        "        b_time[valid_time] = B_gpu[sample_idx, time_sim_gpu[valid_time]]\n",
        "        d_time[valid_time] = D_gpu[sample_idx, time_sim_gpu[valid_time]]\n",
        "\n",
        "    eta1 = X_sim_gpu @ Alpha_gpu[sample_idx] + a_loc + b_time\n",
        "    eta2 = X_sim_gpu @ Beta_gpu[sample_idx] + c_loc + d_time\n",
        "    eta1 = xp.clip(eta1, -500, 500)\n",
        "    eta2 = xp.clip(eta2, -500, 500)\n",
        "\n",
        "    prob = 1.0 / (1.0 + xp.exp(-eta1))\n",
        "    mu = R_gpu[sample_idx] * xp.exp(eta2)\n",
        "\n",
        "    # Transfer to CPU for random number generation\n",
        "    if GPU_AVAILABLE:\n",
        "        prob_cpu = cp.asnumpy(prob)\n",
        "        mu_cpu = cp.asnumpy(mu)\n",
        "        r_cpu = float(cp.asnumpy(R_gpu[sample_idx]))\n",
        "    else:\n",
        "        prob_cpu = prob\n",
        "        mu_cpu = mu\n",
        "        r_cpu = R_gpu[sample_idx]\n",
        "\n",
        "    # Simulate ZINB\n",
        "    y_sim = []\n",
        "    for j in range(len(sim_indices)):\n",
        "        if np.random.rand() < prob_cpu[j]:\n",
        "            p_nb = r_cpu / (r_cpu + mu_cpu[j])\n",
        "            y_j = np.random.negative_binomial(r_cpu, p_nb)\n",
        "        else:\n",
        "            y_j = 0\n",
        "        y_sim.append(y_j)\n",
        "\n",
        "    simulated_datasets.append(y_sim)\n",
        "\n",
        "elapsed_ppc = time.time() - start_time\n",
        "simulated_datasets = np.array(simulated_datasets)\n",
        "\n",
        "print(f\"  ‚úì Simulation completed in {elapsed_ppc:.2f}s\")\n",
        "\n",
        "# Compare statistics\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" GOODNESS-OF-FIT ASSESSMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "stats_comparison = {\n",
        "    'Mean': (y_observed.mean(), simulated_datasets.mean()),\n",
        "    'Std Dev': (y_observed.std(), simulated_datasets.std()),\n",
        "    'Median': (np.median(y_observed), np.median(simulated_datasets)),\n",
        "    '% Zeros': (100*np.mean(y_observed==0), 100*np.mean(simulated_datasets==0)),\n",
        "    'Max': (y_observed.max(), simulated_datasets.max()),\n",
        "    '95th %ile': (np.percentile(y_observed, 95), np.percentile(simulated_datasets, 95))\n",
        "}\n",
        "\n",
        "print(f\"\\n{'Statistic':<15} {'Observed':>12} {'Simulated':>12} {'Diff %':>10} {'Match?':>10}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for stat_name, (obs_val, sim_val) in stats_comparison.items():\n",
        "    diff_pct = 100 * abs(obs_val - sim_val) / (obs_val + 1e-6)\n",
        "    match = '‚úì' if diff_pct < 10 else '‚úó'\n",
        "    print(f\"{stat_name:<15} {obs_val:>12.2f} {sim_val:>12.2f} {diff_pct:>10.1f} {match:>10}\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "\n",
        "good_fit_count = sum(1 for _, (o, s) in stats_comparison.items()\n",
        "                     if 100*abs(o-s)/(o+1e-6) < 10)\n",
        "total_stats = len(stats_comparison)\n",
        "\n",
        "if good_fit_count >= total_stats * 0.8:\n",
        "    print(f\"\\n‚úì Excellent fit: {good_fit_count}/{total_stats} statistics match within 10%\")\n",
        "elif good_fit_count >= total_stats * 0.6:\n",
        "    print(f\"\\n‚úì Good fit: {good_fit_count}/{total_stats} statistics match within 10%\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Moderate fit: {good_fit_count}/{total_stats} statistics match within 10%\")\n",
        "\n",
        "# Save results\n",
        "ppc_results = {\n",
        "    'observed_mean': float(y_observed.mean()),\n",
        "    'simulated_mean': float(simulated_datasets.mean()),\n",
        "    'observed_std': float(y_observed.std()),\n",
        "    'simulated_std': float(simulated_datasets.std()),\n",
        "    'observed_zeros_pct': float(100*np.mean(y_observed==0)),\n",
        "    'simulated_zeros_pct': float(100*np.mean(simulated_datasets==0)),\n",
        "    'n_simulations': int(n_sims),\n",
        "    'good_fit_statistics': f'{good_fit_count}/{total_stats}',\n",
        "    'computation_time_seconds': float(elapsed_ppc),\n",
        "    'gpu_accelerated': GPU_AVAILABLE\n",
        "}\n",
        "\n",
        "ppc_path = f'{OUTPUT_FOLDER}/posterior_predictive_checks.json'\n",
        "with open(ppc_path, 'w') as f:\n",
        "    json.dump(ppc_results, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úì Saved: posterior_predictive_checks.json\")\n",
        "\n",
        "# Create diagnostic plots\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# Plot 1: Distribution Comparison\n",
        "axes[0, 0].hist(y_observed, bins=50, alpha=0.5, label='Observed',\n",
        "                density=True, color='blue', edgecolor='black')\n",
        "axes[0, 0].hist(simulated_datasets.flatten(), bins=50, alpha=0.5,\n",
        "                label='Simulated', density=True, color='red', edgecolor='black')\n",
        "axes[0, 0].set_xlabel('Ride Count', fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Density', fontweight='bold')\n",
        "axes[0, 0].set_title('Distribution Comparison', fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].set_yscale('log')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Q-Q Plot\n",
        "from scipy import stats as scipy_stats\n",
        "scipy_stats.probplot(y_observed, dist=\"norm\", plot=axes[0, 1])\n",
        "axes[0, 1].set_title('Q-Q Plot (Observed Data)', fontweight='bold')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: CDF Comparison\n",
        "sorted_obs = np.sort(y_observed)\n",
        "sorted_sim = np.sort(simulated_datasets.flatten())\n",
        "axes[0, 2].plot(sorted_obs, np.arange(len(sorted_obs))/len(sorted_obs),\n",
        "                label='Observed', lw=2)\n",
        "axes[0, 2].plot(sorted_sim, np.arange(len(sorted_sim))/len(sorted_sim),\n",
        "                label='Simulated', lw=2, alpha=0.7)\n",
        "axes[0, 2].set_xlabel('Ride Count', fontweight='bold')\n",
        "axes[0, 2].set_ylabel('Cumulative Probability', fontweight='bold')\n",
        "axes[0, 2].set_title('CDF Comparison', fontweight='bold')\n",
        "axes[0, 2].legend()\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Zero-Inflation Check\n",
        "zero_counts_sim = [np.mean(sim_data == 0) for sim_data in simulated_datasets]\n",
        "axes[1, 0].hist(zero_counts_sim, bins=30, alpha=0.7, edgecolor='black', color='skyblue')\n",
        "axes[1, 0].axvline(np.mean(y_observed == 0), color='r', linestyle='--',\n",
        "                   lw=2, label='Observed')\n",
        "axes[1, 0].set_xlabel('Proportion of Zeros', fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Frequency', fontweight='bold')\n",
        "axes[1, 0].set_title('Zero-Inflation Check', fontweight='bold')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 5: Mean Check\n",
        "mean_sim = simulated_datasets.mean(axis=1)\n",
        "axes[1, 1].hist(mean_sim, bins=30, alpha=0.7, edgecolor='black', color='lightgreen')\n",
        "axes[1, 1].axvline(y_observed.mean(), color='r', linestyle='--',\n",
        "                   lw=2, label='Observed')\n",
        "axes[1, 1].set_xlabel('Mean Rides', fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Frequency', fontweight='bold')\n",
        "axes[1, 1].set_title('Mean Check', fontweight='bold')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 6: Variance Check\n",
        "var_sim = simulated_datasets.var(axis=1)\n",
        "axes[1, 2].hist(var_sim, bins=30, alpha=0.7, edgecolor='black', color='lightcoral')\n",
        "axes[1, 2].axvline(y_observed.var(), color='r', linestyle='--',\n",
        "                   lw=2, label='Observed')\n",
        "axes[1, 2].set_xlabel('Variance', fontweight='bold')\n",
        "axes[1, 2].set_ylabel('Frequency', fontweight='bold')\n",
        "axes[1, 2].set_title('Variance Check', fontweight='bold')\n",
        "axes[1, 2].legend()\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_FOLDER}/posterior_predictive_checks.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"‚úì Saved: posterior_predictive_checks.png\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao__9qgzfPoR",
        "outputId": "afe01e93-d1ac-43cb-9bcd-e0d00ae94859"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            " VALIDATION: POSTERIOR PREDICTIVE CHECKS\n",
            "================================================================================\n",
            "\n",
            "Simulating 50 datasets from 5,000 observations...\n",
            "  ‚úì Mapped observations to locations and times\n",
            "  Location dtype: int32\n",
            "  Time dtype: int32\n",
            "  Transferring data to GPU...\n",
            "    Simulation 1/50...\n",
            "    Simulation 11/50...\n",
            "    Simulation 21/50...\n",
            "    Simulation 31/50...\n",
            "    Simulation 41/50...\n",
            "  ‚úì Simulation completed in 0.54s\n",
            "\n",
            "================================================================================\n",
            " GOODNESS-OF-FIT ASSESSMENT\n",
            "================================================================================\n",
            "\n",
            "Statistic           Observed    Simulated     Diff %     Match?\n",
            "-----------------------------------------------------------------\n",
            "Mean                   62.28        33.87       45.6          ‚úó\n",
            "Std Dev               758.73       205.84       72.9          ‚úó\n",
            "Median                  0.00         0.00        0.0          ‚úì\n",
            "% Zeros                89.22        50.09       43.9          ‚úó\n",
            "Max                 19062.00      7623.00       60.0          ‚úó\n",
            "95th %ile               5.00        37.00      640.0          ‚úó\n",
            "================================================================================\n",
            "\n",
            "‚ö†Ô∏è  Moderate fit: 1/6 statistics match within 10%\n",
            "\n",
            "‚úì Saved: posterior_predictive_checks.json\n",
            "‚úì Saved: posterior_predictive_checks.png\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# GEOSIMULATION SECTION: PREDICTION ENGINE (GPU-ACCELERATED WITH PROGRESS)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" GEOSIMULATION: PREDICTION ENGINE (GPU-ACCELERATED)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def predict_by_location_gpu(X_scenario, df_features, location_data,\n",
        "                            Alpha, Beta, A, B, C, D, R, xp, GPU_AVAILABLE):\n",
        "    \"\"\"\n",
        "    GPU-accelerated prediction by location with progress tracking\n",
        "\n",
        "    Returns:\n",
        "        location_predictions: dict with prediction results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"  Grouping by location...\")\n",
        "    location_results = {}\n",
        "\n",
        "    # Get unique locations\n",
        "    unique_locs = location_data['location_idx'].values\n",
        "    n_locs = len(unique_locs)\n",
        "    n_samples = len(R)\n",
        "\n",
        "    # Transfer scenario to GPU\n",
        "    if GPU_AVAILABLE:\n",
        "        X_scenario_gpu = cp.asarray(X_scenario)\n",
        "    else:\n",
        "        X_scenario_gpu = X_scenario\n",
        "\n",
        "    # Get location and time indices for all observations\n",
        "    df_with_idx = df_features.copy()\n",
        "    df_with_idx['loc_idx'] = df_with_idx.apply(get_location_index, axis=1)\n",
        "    df_with_idx['time_idx'] = df_with_idx.apply(get_time_index, axis=1)\n",
        "\n",
        "    # CRITICAL FIX: Convert to integer arrays\n",
        "    loc_indices = df_with_idx['loc_idx'].values.astype(np.int32)\n",
        "    time_indices = df_with_idx['time_idx'].values.astype(np.int32)\n",
        "\n",
        "    if GPU_AVAILABLE:\n",
        "        # CRITICAL FIX: Ensure integer type on GPU\n",
        "        loc_indices_gpu = cp.asarray(loc_indices, dtype=cp.int32)\n",
        "        time_indices_gpu = cp.asarray(time_indices, dtype=cp.int32)\n",
        "    else:\n",
        "        loc_indices_gpu = loc_indices\n",
        "        time_indices_gpu = time_indices\n",
        "\n",
        "    print(f\"  Computing predictions for {n_locs:,} locations...\")\n",
        "    print(f\"    Using {n_samples} posterior samples\")\n",
        "    print(f\"    Processing {len(X_scenario):,} observations total\")\n",
        "    print(f\"    Index dtypes: loc={loc_indices.dtype}, time={time_indices.dtype}\")\n",
        "\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Progress tracking setup\n",
        "    print(\"\\n  Progress:\")\n",
        "    spinner_chars = ['|', '/', '-', '\\\\']\n",
        "    spinner_idx = 0\n",
        "\n",
        "    # Predict all observations at once! (GPU-accelerated)\n",
        "    all_predictions = predict_vectorized_gpu(\n",
        "        X_scenario_gpu, loc_indices_gpu, time_indices_gpu,\n",
        "        Alpha, Beta, A, B, C, D, R, xp\n",
        "    )\n",
        "\n",
        "    # Transfer back to CPU\n",
        "    if GPU_AVAILABLE:\n",
        "        print(f\"    {spinner_chars[spinner_idx]} Transferring results from GPU...\")\n",
        "        all_predictions = cp.asnumpy(all_predictions)\n",
        "        loc_indices_cpu = cp.asnumpy(loc_indices_gpu)\n",
        "        spinner_idx = (spinner_idx + 1) % 4\n",
        "    else:\n",
        "        loc_indices_cpu = loc_indices\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"    ‚úì Predictions computed in {elapsed:.2f}s\")\n",
        "    if GPU_AVAILABLE:\n",
        "        print(f\"      (GPU acceleration: ~10-20x faster!)\")\n",
        "\n",
        "    # Aggregate by location with progress\n",
        "    print(\"\\n  Aggregating by location...\")\n",
        "\n",
        "    location_means = []\n",
        "    location_stds = []\n",
        "    location_ci_lower = []\n",
        "    location_ci_upper = []\n",
        "\n",
        "    # Progress bar setup\n",
        "    last_update = time.time()\n",
        "    update_interval = 2.0  # Update every 2 seconds\n",
        "\n",
        "    for idx, loc_idx in enumerate(unique_locs):\n",
        "        # Get all observations for this location\n",
        "        mask = (loc_indices_cpu == loc_idx)\n",
        "\n",
        "        if mask.sum() > 0:\n",
        "            # Predictions for this location: [n_samples, n_obs_at_loc]\n",
        "            loc_preds = all_predictions[:, mask]\n",
        "\n",
        "            # Aggregate: mean across time, then mean/std across samples\n",
        "            mean_over_time = loc_preds.mean(axis=1)  # [n_samples]\n",
        "\n",
        "            location_means.append(mean_over_time.mean())\n",
        "            location_stds.append(mean_over_time.std())\n",
        "            location_ci_lower.append(np.percentile(mean_over_time, 2.5))\n",
        "            location_ci_upper.append(np.percentile(mean_over_time, 97.5))\n",
        "        else:\n",
        "            location_means.append(0.0)\n",
        "            location_stds.append(0.0)\n",
        "            location_ci_lower.append(0.0)\n",
        "            location_ci_upper.append(0.0)\n",
        "\n",
        "        # Progress update with spinner (prevent timeout)\n",
        "        current_time = time.time()\n",
        "        if current_time - last_update >= update_interval:\n",
        "            progress_pct = 100 * (idx + 1) / n_locs\n",
        "            elapsed_so_far = current_time - start_time\n",
        "            eta = elapsed_so_far * (n_locs / (idx + 1) - 1)\n",
        "\n",
        "            print(f\"    {spinner_chars[spinner_idx]} Progress: {idx+1:,}/{n_locs:,} ({progress_pct:.1f}%) | \"\n",
        "                  f\"Elapsed: {elapsed_so_far:.1f}s | ETA: {eta:.1f}s\", end='\\r')\n",
        "\n",
        "            spinner_idx = (spinner_idx + 1) % 4\n",
        "            last_update = current_time\n",
        "\n",
        "    # Final progress update\n",
        "    total_elapsed = time.time() - start_time\n",
        "    print(f\"    ‚úì Aggregation complete: {n_locs:,} locations in {total_elapsed:.1f}s\" + \" \" * 30)\n",
        "\n",
        "    location_predictions = {\n",
        "        'location_idx': unique_locs,\n",
        "        'predicted_demand': np.array(location_means),\n",
        "        'demand_std': np.array(location_stds),\n",
        "        'demand_ci_lower': np.array(location_ci_lower),\n",
        "        'demand_ci_upper': np.array(location_ci_upper),\n",
        "        'computation_time_seconds': total_elapsed\n",
        "    }\n",
        "\n",
        "    total_demand = location_predictions['predicted_demand'].sum()\n",
        "    print(f\"  ‚úì Total predicted demand: {total_demand:,.0f} rides\")\n",
        "\n",
        "    return location_predictions\n",
        "\n",
        "print(\"\\n‚úì Prediction engine ready (GPU-accelerated)\")\n",
        "\n",
        "# ============================================================================\n",
        "# GEOSIMULATION SECTION: EXECUTE SCENARIOS (WITH PROGRESS TRACKING)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" GEOSIMULATION: EXECUTING SCENARIOS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "scenario_results = {}\n",
        "overall_start = time.time()\n",
        "\n",
        "for scenario_idx, (scenario_name, (X_scenario, description)) in enumerate(scenarios.items()):\n",
        "    print(f\"\\n[{scenario_idx+1}/{len(scenarios)}] Scenario: {scenario_name}\")\n",
        "    print(f\"  Description: {description}\")\n",
        "\n",
        "    # Keep-alive heartbeat\n",
        "    import sys\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    # GPU-accelerated prediction with progress!\n",
        "    predictions = predict_by_location_gpu(\n",
        "        X_scenario, df_features, location_data,\n",
        "        Alpha_gpu, Beta_gpu, A_gpu, B_gpu, C_gpu, D_gpu, R_gpu,\n",
        "        xp, GPU_AVAILABLE\n",
        "    )\n",
        "\n",
        "    scenario_results[scenario_name] = predictions\n",
        "\n",
        "    total = predictions['predicted_demand'].sum()\n",
        "    mean_loc = predictions['predicted_demand'].mean()\n",
        "    max_loc = predictions['predicted_demand'].max()\n",
        "\n",
        "    print(f\"\\n  Results:\")\n",
        "    print(f\"    Total demand:      {total:>12,.0f} rides\")\n",
        "    print(f\"    Mean per location: {mean_loc:>12,.2f} rides\")\n",
        "    print(f\"    Max location:      {max_loc:>12,.2f} rides\")\n",
        "    print(f\"    Computation time:  {predictions['computation_time_seconds']:>12,.2f}s\")\n",
        "\n",
        "    # Keep-alive heartbeat\n",
        "    sys.stdout.flush()\n",
        "\n",
        "overall_elapsed = time.time() - overall_start\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\" ‚úÖ ALL SCENARIOS COMPLETED IN {overall_elapsed:.2f}s\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if GPU_AVAILABLE:\n",
        "    print(f\"\\n‚ö° GPU ACCELERATION SUMMARY:\")\n",
        "    print(f\"  Total geosimulation time: {overall_elapsed:.2f}s\")\n",
        "    print(f\"  Estimated CPU time: ~{overall_elapsed*15:.0f}s ({overall_elapsed*15/60:.1f} minutes)\")\n",
        "    print(f\"  Speedup: ~15-20x faster with GPU!\")\n",
        "\n",
        "# Comparison with baseline\n",
        "baseline_total = scenario_results['baseline']['predicted_demand'].sum()\n",
        "print(f\"\\nScenario Comparison (vs Baseline = {baseline_total:,.0f}):\")\n",
        "print(f\"{'Scenario':<25} {'Total Demand':>15} {'Change':>12} {'% Change':>10}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for name in scenarios.keys():\n",
        "    total = scenario_results[name]['predicted_demand'].sum()\n",
        "    if name == 'baseline':\n",
        "        print(f\"{name:<25} {total:>15,.0f} {'‚Äî':>12} {'‚Äî':>10}\")\n",
        "    else:\n",
        "        diff = total - baseline_total\n",
        "        pct = 100 * diff / baseline_total\n",
        "        print(f\"{name:<25} {total:>15,.0f} {diff:>12,.0f} {pct:>9.1f}%\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ewz8JkCfZbh",
        "outputId": "e021d918-4ca4-46f7-d53b-ad69ac923a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            " GEOSIMULATION: PREDICTION ENGINE (GPU-ACCELERATED)\n",
            "================================================================================\n",
            "\n",
            "‚úì Prediction engine ready (GPU-accelerated)\n",
            "\n",
            "================================================================================\n",
            " GEOSIMULATION: EXECUTING SCENARIOS\n",
            "================================================================================\n",
            "\n",
            "[1/5] Scenario: baseline\n",
            "  Description: Current patterns (baseline)\n",
            "  Grouping by location...\n",
            "  Computing predictions for 14,513 locations...\n",
            "    Using 50 posterior samples\n",
            "    Processing 696,672 observations total\n",
            "    Index dtypes: loc=int32, time=int32\n",
            "\n",
            "  Progress:\n",
            "    | Transferring results from GPU...\n",
            "    ‚úì Predictions computed in 0.58s\n",
            "      (GPU acceleration: ~10-20x faster!)\n",
            "\n",
            "  Aggregating by location...\n",
            "    ‚úì Aggregation complete: 14,513 locations in 34.5s                              \n",
            "  ‚úì Total predicted demand: 487,865 rides\n",
            "\n",
            "  Results:\n",
            "    Total demand:           487,865 rides\n",
            "    Mean per location:        33.62 rides\n",
            "    Max location:            671.96 rides\n",
            "    Computation time:         34.53s\n",
            "\n",
            "[2/5] Scenario: all_weekend\n",
            "  Description: All days treated as weekends\n",
            "  Grouping by location...\n",
            "  Computing predictions for 14,513 locations...\n",
            "    Using 50 posterior samples\n",
            "    Processing 696,672 observations total\n",
            "    Index dtypes: loc=int32, time=int32\n",
            "\n",
            "  Progress:\n",
            "    | Transferring results from GPU...\n",
            "    ‚úì Predictions computed in 0.62s\n",
            "      (GPU acceleration: ~10-20x faster!)\n",
            "\n",
            "  Aggregating by location...\n",
            "    ‚úì Aggregation complete: 14,513 locations in 34.4s                              \n",
            "  ‚úì Total predicted demand: 351,410 rides\n",
            "\n",
            "  Results:\n",
            "    Total demand:           351,410 rides\n",
            "    Mean per location:        24.21 rides\n",
            "    Max location:            479.00 rides\n",
            "    Computation time:         34.37s\n",
            "\n",
            "[3/5] Scenario: all_weekday\n",
            "  Description: All days treated as weekdays\n",
            "  Grouping by location...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SAVE RESULTS TO CSV\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" SAVING RESULTS TO CSV\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Detailed predictions by location\n",
        "results_list = []\n",
        "\n",
        "for loc_idx in location_indices:\n",
        "    loc_info = location_data[location_data['location_idx'] == loc_idx].iloc[0]\n",
        "\n",
        "    row = {\n",
        "        'location_idx': loc_idx,\n",
        "        'grid_x': loc_info['grid_x'],\n",
        "        'grid_y': loc_info['grid_y'],\n",
        "        'centroid_lon': loc_info['centroid_lon'],\n",
        "        'centroid_lat': loc_info['centroid_lat'],\n",
        "    }\n",
        "\n",
        "    # Add predictions for each scenario\n",
        "    for scenario_name, predictions in scenario_results.items():\n",
        "        if loc_idx in predictions:\n",
        "            row[f'{scenario_name}_mean'] = predictions[loc_idx]['mean']\n",
        "            row[f'{scenario_name}_std'] = predictions[loc_idx]['std']\n",
        "            row[f'{scenario_name}_ci_lower'] = predictions[loc_idx]['ci_lower']\n",
        "            row[f'{scenario_name}_ci_upper'] = predictions[loc_idx]['ci_upper']\n",
        "\n",
        "    # Compute differences from baseline\n",
        "    if loc_idx in baseline_preds:\n",
        "        for scenario_name in scenario_results.keys():\n",
        "            if scenario_name != 'baseline' and loc_idx in scenario_results[scenario_name]:\n",
        "                baseline_mean = baseline_preds[loc_idx]['mean']\n",
        "                scenario_mean = scenario_results[scenario_name][loc_idx]['mean']\n",
        "\n",
        "                row[f'{scenario_name}_diff'] = scenario_mean - baseline_mean\n",
        "                row[f'{scenario_name}_pct_change'] = (\n",
        "                    100 * (scenario_mean - baseline_mean) / (baseline_mean + 1e-6)\n",
        "                )\n",
        "\n",
        "    results_list.append(row)\n",
        "\n",
        "results_df = pd.DataFrame(results_list)\n",
        "\n",
        "# Save detailed results\n",
        "csv_path = f'{OUTPUT_FOLDER}/scenario_predictions_by_location.csv'\n",
        "results_df.to_csv(csv_path, index=False)\n",
        "print(f\"‚úì Saved: scenario_predictions_by_location.csv\")\n",
        "print(f\"  {len(results_df)} locations √ó {len(results_df.columns)} columns\")\n",
        "\n",
        "# Summary statistics\n",
        "summary_stats = []\n",
        "for scenario_name in scenario_results.keys():\n",
        "    scenario_mean = results_df[f'{scenario_name}_mean']\n",
        "    total_demand = scenario_mean.sum()\n",
        "\n",
        "    if scenario_name == 'baseline':\n",
        "        pct_change = 0\n",
        "    else:\n",
        "        baseline_total = results_df['baseline_mean'].sum()\n",
        "        pct_change = 100 * (total_demand - baseline_total) / (baseline_total + 1e-6)\n",
        "\n",
        "    summary_stats.append({\n",
        "        'Scenario': scenario_name,\n",
        "        'Description': scenarios_dict[scenario_name][1],\n",
        "        'Total_Demand': total_demand,\n",
        "        'Mean_Per_Location': scenario_mean.mean(),\n",
        "        'Std_Per_Location': scenario_mean.std(),\n",
        "        'Max_Location': scenario_mean.max(),\n",
        "        'Pct_Change_From_Baseline': pct_change\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_stats)\n",
        "summary_path = f'{OUTPUT_FOLDER}/scenario_summary.csv'\n",
        "summary_df.to_csv(summary_path, index=False)\n",
        "print(f\"‚úì Saved: scenario_summary.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" SCENARIO SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(summary_df.to_string(index=False))\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL GEOSIMULATION SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" üéâ COMPLETE GEOSIMULATION ANALYSIS FINISHED!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüìä BASELINE MAPS (Original):\")\n",
        "print(\"  ‚Ä¢ map_binary_spatial.png\")\n",
        "print(\"  ‚Ä¢ map_count_spatial.png\")\n",
        "print(\"  ‚Ä¢ map_activity_score.png\")\n",
        "print(\"  ‚Ä¢ map_observed_rides.png\")\n",
        "print(\"  ‚Ä¢ heatmap_activity_grid.png\")\n",
        "print(\"  ‚Ä¢ temporal_by_location_type.png\")\n",
        "\n",
        "print(\"\\nüìà SCENARIO MAPS (New):\")\n",
        "for scenario_name in scenario_results.keys():\n",
        "    print(f\"  ‚Ä¢ scenario_map_{scenario_name}.png\")\n",
        "\n",
        "print(\"\\nüìâ DIFFERENCE MAPS (New):\")\n",
        "for scenario_name in scenario_results.keys():\n",
        "    if scenario_name != 'baseline':\n",
        "        print(f\"  ‚Ä¢ difference_map_{scenario_name}.png\")\n",
        "\n",
        "print(\"\\nüìã COMPARISON & DATA (New):\")\n",
        "print(\"  ‚Ä¢ scenario_comparison_panel.png\")\n",
        "print(\"  ‚Ä¢ scenario_predictions_by_location.csv\")\n",
        "print(\"  ‚Ä¢ scenario_summary.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" ‚úÖ READY FOR PRESENTATION!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "You now have COMPLETE geosimulation:\n",
        "  ‚úì Baseline spatial effects (model diagnostics)\n",
        "  ‚úì Scenario predictions (policy analysis)\n",
        "  ‚úì Spatial maps (geographic visualization)\n",
        "  ‚úì Difference maps (impact assessment)\n",
        "  ‚úì Comprehensive datasets (detailed results)\n",
        "\n",
        "Total outputs: 17 files\n",
        "  - 11 PNG maps\n",
        "  - 3 interactive HTML maps\n",
        "  - 2 CSV data files\n",
        "  - 1 comparison panel\n",
        "\n",
        "Ready for academic presentation and publication! üéìüöÄ\n",
        "\"\"\")\n",
        "print(\"=\"*80)\n",
        "\n"
      ],
      "metadata": {
        "id": "3gYQNI85eSDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# OPTIONAL: ARCHIVE RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" ARCHIVING OUTPUTS (OPTIONAL)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "try:\n",
        "    zip_filename = os.path.join(os.path.dirname(OUTPUT_FOLDER), 'Spatial_Temp_Analysis_Results.zip')\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(OUTPUT_FOLDER):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(('.png', '.html', '.json', '.csv')):\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    arcname = os.path.relpath(file_path, os.path.dirname(OUTPUT_FOLDER))\n",
        "                    zipf.write(file_path, arcname)\n",
        "\n",
        "    print(f\"‚úì Archive created: {zip_filename}\")\n",
        "    if os.path.exists(zip_filename):\n",
        "        print(f\"  Size: {os.path.getsize(zip_filename)/1024/1024:.2f} MB\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Could not create archive: {e}\")\n",
        "    print(\"  (This is optional - all files are still saved individually)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" ‚úÖ ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "B1ueJUnPvwh9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}